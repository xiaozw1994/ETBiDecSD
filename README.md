# ETBiDecSD: Ensemble Transitive Bidirectional Decoupled Self-Distillation for Time Series Classification

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.6%2B-blue)](https://www.python.org/)
[![Pytorch](https://img.shields.io/badge/PyTorch-1.3%2B-orange)](https://pytorch.org/)

This repository provides the implementation of ETBiDecSD ("Ensemble Transitive Bidirectional Decoupled Self-Distillation for Time Series Classification"), a novel approach for time series classification that combines ensemble learning with bidirectional knowledge distillation.

ðŸ“„ **Paper Status**: Submitted to IEEE Transactions on Systems, Man, and Cybernetics: Systems Journal


## ðŸ“¥ Dataset Preparation

### UCR Archive Datasets
1. Download datasets from [UCR Time Series Classification Archive](http://timeseriesclassification.com/dataset.php)
2. Unzip the downloaded datasets
3. Place them in the `UCRData` folder with the following structure:

## ðŸ›  Installation

1. Clone this repository:
```
git clone https://github.com/yourusername/ETBiDecSD.git
cd ETBiDecSD


2. Install required packages:

'''
pip install -r requirements.txt


## ðŸ“‚ Implementation Repository Structure
'''
ETBiDecSD/
â”œâ”€â”€ UCRData/ # Dataset directory
â”œâ”€â”€ network/
â”‚ â”œâ”€â”€ network.py # Model architecture
â”‚ â””â”€â”€ decouple_loss.py # Custom loss functions
â”œâ”€â”€ processing/
â”‚ â”œâ”€â”€ config.py # Configuration
â”‚ â”œâ”€â”€ data.py # Data processing
â”‚ â””â”€â”€ augmentation.py # Data augmentation
â”œâ”€â”€ main.py # Main implementation script (e.g., Train and Test)
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # This file







{
Authors="Zhiwen Xiao, Huanlai Xing, Rong Qu, Hui Li, Li Feng, Bowen Zhao, and Qian Wan",
Title = "Ensemble Transitive Bidirectional Decoupled Self-Distillation for Time Series Classification,"
}
